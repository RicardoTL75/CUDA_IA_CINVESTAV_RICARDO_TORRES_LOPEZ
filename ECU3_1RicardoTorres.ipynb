{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOsLV8CtRE+SiawOzd5SRs3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_qM50k9huhc_","executionInfo":{"status":"ok","timestamp":1763498966530,"user_tz":420,"elapsed":9182,"user":{"displayName":"Ricardo Torres","userId":"06553239871648539140"}},"outputId":"20b7b84a-998a-44c6-dd97-3d57c246e80a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pynvjitlink-cu12\n","  Downloading pynvjitlink_cu12-0.7.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Downloading pynvjitlink_cu12-0.7.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (46.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pynvjitlink-cu12\n","Successfully installed pynvjitlink-cu12-0.7.0\n"]}],"source":["# @title\n","!uv pip install -q --system numba-cuda==0.4.0\n","!pip install pynvjitlink-cu12\n","\n","import numpy as np\n","from numba import cuda\n","import time\n","import os\n","from numba import config\n","import numba\n","config.CUDA_ENABLE_PYNVJITLINK=1"]},{"cell_type":"code","source":["# @title\n","# ex1_vector_add.py\n","import numpy as np\n","from numba import cuda\n","import math\n","import time\n","\n","@cuda.jit\n","def vector_add_kernel(a, b, c):\n","    \"\"\"\n","    Each thread computes one element: c[i] = a[i] + b[i]\n","    \"\"\"\n","\n","    # Compute global thread index\n","    idx = cuda.grid(1)\n","\n","    # Boundary check\n","    if idx < c.size:\n","        c[idx] = a[idx] + b[idx]\n","\n","def main():\n","    N_large = 10_000_000\n","\n","    a = np.random.randn(N_large).astype(np.float32)\n","    b = np.random.randn(N_large).astype(np.float32)\n","    c = np.zeros(N_large, dtype=np.float32)\n","\n","    d_a = cuda.to_device(a)\n","    d_b = cuda.to_device(b)\n","    d_c = cuda.to_device(c)\n","\n","    threads_per_block = 256\n","    blocks_per_grid = math.ceil(N_large / threads_per_block)\n","\n","    # Warmup\n","    vector_add_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n","    cuda.synchronize()\n","\n","    # Run timed kernel\n","    start = time.time()\n","    vector_add_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n","    cuda.synchronize()\n","    gpu_time = (time.time() - start) * 1000\n","\n","    print(f\"GPU time: {gpu_time:.2f} ms\")\n","\n","    # Copy result to CPU\n","    result = d_c.copy_to_host()\n","\n","    # CPU timing\n","    cpu_start = time.time()\n","    expected =  a + b\n","    cpu_time = (time.time() - cpu_start) * 1000\n","\n","    print(f\"GPU Kernel Time: {gpu_time:.3f} ms\")\n","    print(f\"CPU Numpy Time: {cpu_time:.3f} ms\")\n","    print(f\"Speedup:{cpu_time / gpu_time:.2f}x\")\n","    print(f\"Correct:\", np.allclose(result, expected))\n","\n","\"\"\"\n","    # Optional: validate correctness\n","    if np.allclose(result, a + b):\n","        print(\"Vector add correct!\")\n","    else:\n","        print(\"Error in GPU computation.\")\n","\"\"\"\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"cellView":"form","id":"TE3mOmrKvAvp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","import numpy as np\n","from numba import cuda\n","import math\n","import time\n","\n","@numba.cuda.jit\n","def dummy_compute_kernel(a, b, c):\n","    \"\"\"\n","    Simple compute to measure timing: c[i] = sqrt(a[i]^2 + b[i]^2)\n","    \"\"\"\n","    idx = cuda.grid(1)\n","    if idx < c.size:\n","        c[idx] = math.sqrt(a[idx]**2 + b[idx]**2)\n","\n","def main():\n","    N_large = 10_000_000\n","\n","    a = np.random.randn(N_large).astype(np.float32)\n","    b = np.random.randn(N_large).astype(np.float32)\n","    c = np.zeros(N_large, dtype=np.float32)\n","\n","    d_a = cuda.to_device(a)\n","    d_b = cuda.to_device(b)\n","    d_c = cuda.to_device(c)\n","\n","    threads_per_block = 256\n","    blocks_per_grid = math.ceil(N_large / threads_per_block)\n","\n","    # Warmup\n","    dummy_compute_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n","    cuda.synchronize()\n","\n","    # Run timed kernel\n","    start = time.time()\n","    dummy_compute_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n","    cuda.synchronize()\n","    gpu_time = (time.time() - start) * 1000\n","\n","    print(f\"GPU time: {gpu_time:.2f} ms\")\n","\n","    # Copy result to CPU\n","    result = d_c.copy_to_host()\n","\n","    # CPU timing\n","    cpu_start = time.time()\n","    expected = np.sqrt(a**2 + b**2)\n","    cpu_end = time.time()\n","    cpu_time = (cpu_end - cpu_start) * 1000\n","\n","    print(f\"GPU Kernel Time: {gpu_time:.3f} ms\")\n","    print(f\"CPU Numpy Time: {cpu_time:.3f} ms\")\n","    print(f\"Speedup:{cpu_time / gpu_time:.2f}x\")\n","    print(f\"Correct:\", np.allclose(result, expected))\n","\n","\"\"\"\n","    # Optional: validate correctness\n","    if np.allclose(result, a + b):\n","        print(\"Vector add correct!\")\n","    else:\n","        print(\"Error in GPU computation.\")\n","\"\"\"\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"cellView":"form","id":"K7ey_JDMvfJf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","import numpy as np\n","from numba import cuda\n","import math\n","import time\n","\n","@cuda.jit\n","def matrix_scale_kernel(mat, scalar, out):\n","    \"\"\"\n","    Escala cada elemento:\n","    out[row, col] = mat[row, col] * scalar\n","    \"\"\"\n","    row, col = cuda.grid(2)\n","\n","    if row < out.shape[0] and col < out.shape[1]:\n","        out[row, col] = mat[row, col] * scalar\n","\n","def main():\n","\n","    rows_large, cols_large = 4096, 4096\n","\n","    # CORRECCIÓN: np.random.rand\n","    mat = np.random.rand(rows_large, cols_large).astype(np.float32)\n","    out = np.zeros_like(mat)\n","    scalar = 2.5\n","\n","    # Copia GPU\n","    d_mat = cuda.to_device(mat)\n","    d_out = cuda.to_device(out)\n","\n","    # Configuración CUDA 2D\n","    threads_per_block = (32, 32)\n","    blocks_per_grid_x = math.ceil(rows_large / threads_per_block[0])\n","    blocks_per_grid_y = math.ceil(cols_large / threads_per_block[1])\n","    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n","\n","    # Warmup\n","    matrix_scale_kernel[blocks_per_grid, threads_per_block](d_mat, scalar, d_out)\n","    cuda.synchronize()\n","\n","    # Timed kernel\n","    start = time.time()\n","    matrix_scale_kernel[blocks_per_grid, threads_per_block](d_mat, scalar, d_out)\n","    cuda.synchronize()\n","    gpu_time = (time.time() - start) * 1000\n","\n","    # CORRECCIÓN: agregar paréntesis\n","    result = d_out.copy_to_host()\n","\n","    # CPU timing\n","    cpu_start = time.time()\n","    expected = mat * scalar\n","    cpu_time = (time.time() - cpu_start) * 1000\n","\n","    print(f\"GPU Kernel Time: {gpu_time:.3f} ms\")\n","    print(f\"CPU Numpy Time: {cpu_time:.3f} ms\")\n","    print(f\"Speedup: {cpu_time / gpu_time:.2f}x\")\n","    print(\"Correct:\", np.allclose(result, expected))\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"cellView":"form","id":"eGoODPUSvqzk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","import numpy as np\n","from numba import cuda\n","import math\n","import time\n","\n","\n","@cuda.jit\n","def matmul_no_for(A, B, C):\n","    \"\"\"\n","    Multiplicación de matrices C = A x B sin usar for en Python.\n","    Cada hilo calcula un producto parcial A[row, k] * B[k, col]\n","    y lo suma atómicamente en C[row, col].\n","    \"\"\"\n","\n","    # Índices globales en grid 3D\n","    row, col, k = cuda.grid(3)\n","\n","    M, K = A.shape       # A: M x K\n","    K2, N = B.shape      # B: K2 x N\n","\n","    # Verificamos que las dimensiones sean compatibles\n","    if K2 != K:\n","        return\n","\n","    # Verificación de límites\n","    if row < M and col < N and k < K:\n","        val = A[row, k] * B[k, col]\n","        # Suma atómica en C[row, col]\n","        cuda.atomic.add(C, (row, col), val)\n","\n","\n","def main():\n","    # Tamaño de matrices: A (M x K), B (K x N), C (M x N)\n","    M, K, N = 512, 512, 512\n","\n","    # Matrices en CPU\n","    A = np.random.randn(M, K).astype(np.float32)\n","    B = np.random.randn(K, N).astype(np.float32)\n","    C = np.zeros((M, N), dtype=np.float32)\n","\n","    # Copia a GPU\n","    d_A = cuda.to_device(A)\n","    d_B = cuda.to_device(B)\n","    d_C = cuda.to_device(C)  # inicializada en ceros\n","\n","    # Configuración del grid 3D y bloques 3D\n","    threads_per_block = (8, 8, 8)  # (tx, ty, tz) → 8*8*8 = 512 hilos por bloque\n","    blocks_per_grid_x = math.ceil(M / threads_per_block[0])\n","    blocks_per_grid_y = math.ceil(N / threads_per_block[1])\n","    blocks_per_grid_z = math.ceil(K / threads_per_block[2])\n","    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y, blocks_per_grid_z)\n","\n","    # Warmup\n","    matmul_no_for[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n","    cuda.synchronize()\n","\n","    # Tiempo GPU\n","    start = time.time()\n","    matmul_no_for[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n","    cuda.synchronize()\n","    gpu_time = (time.time() - start) * 1000\n","\n","    # Copiar resultado a CPU\n","    result = d_C.copy_to_host()\n","\n","    # Tiempo CPU (NumPy)\n","    cpu_start = time.time()\n","    expected = A @ B        # equivalente a np.dot(A, B)\n","    cpu_time = (time.time() - cpu_start) * 1000\n","\n","    print(f\"GPU Kernel Time: {gpu_time:.3f} ms\")\n","    print(f\"CPU Numpy Time: {cpu_time:.3f} ms\")\n","    print(f\"Speedup: {cpu_time / gpu_time:.2f}x\")\n","    print(\"Correct:\", np.allclose(result, expected, rtol=1e-3, atol=1e-3))\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"cellView":"form","id":"34JAsJ4-v6as"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","# Matrix Multiplication\n","# Matrix Multiplication\n","import numpy as np\n","import numba.cuda as cuda\n","import math\n","import time\n","\n","# C = A @ B\n","\n","@numba.cuda.jit\n","def matmul_naive_kernel(A, B, C):\n","    \"\"\"\n","    Naive matrix multiply: C = A @ B\n","    Each thread computes one element of C.\n","    All reads from A and B go to global memory (slow). Alter: shared memory\n","\n","    A: (M, K)\n","    B: (K, N)\n","    C: (M, N)\n","    \"\"\"\n","    row, col = cuda.grid(2)\n","\n","    M, K = A.shape\n","    K2, N = B.shape\n","\n","    if row < M and col < N:\n","      total = 0.0\n","      for k in range(K):\n","          total += A[row, k] * B[k, col]\n","      C[row, col] = total\n","\n","def main():\n","    M, K, N = 1000, 1000, 1000\n","    A = np.random.randn(M, K).astype(np.float32)\n","    B = np.random.randn(K, N).astype(np.float32)\n","    C = np.zeros((M, N), dtype=np.float32)\n","\n","    threads_per_block = (32, 32)\n","    d_A = cuda.to_device(A)\n","    d_B = cuda.to_device(B)\n","    d_C = cuda.to_device(C)\n","\n","    blocks_per_grid_x = (M + threads_per_block[0] - 1) // threads_per_block[0]\n","    blocks_per_grid_y = (N + threads_per_block[1] - 1) // threads_per_block[1]\n","    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n","\n","    # Warmup\n","    matmul_naive_kernel[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n","    cuda.synchronize()\n","\n","    # GPU timing\n","    start = time.time()\n","    matmul_naive_kernel[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n","    cuda.synchronize()\n","    gpu_time = (time.time() - start) * 1000\n","\n","    C_gpu = d_C.copy_to_host()\n","\n","    # CPU timing\n","    cpu_start = time.time()\n","    C_cpu = A @ B\n","    cpu_time = (time.time() - cpu_start) * 1000\n","\n","    print(f\"GPU kernel time: {gpu_time:.4f} ms\")\n","    print(f\"CPU NumPy time: {cpu_time:.4f} ms\")\n","    print(f\"Speedup: {cpu_time / gpu_time:.2f}x\")\n","    print(f\"Correct: {np.allclose(C_gpu, C_cpu, atol=1e-3)}\")\n","\n","main()"],"metadata":{"cellView":"form","id":"yIuzm4F9wG_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","import numpy as np\n","import numba.cuda as cuda\n","import time\n","import cv2\n","import urllib.request\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","\n","# ---------------------------------------------------------\n","# KERNEL CUDA – Sobel\n","# ---------------------------------------------------------\n","@cuda.jit\n","def sobel_kernel(img, out):\n","    \"\"\"\n","    Apply Sobel edge detection - each thread processes one pixel.\n","    \"\"\"\n","    row, col = cuda.grid(2)   # Each thread → one pixel\n","\n","    H, W = img.shape\n","\n","    # Avoid borders\n","    if 0 < row < H-1 and 0 < col < W-1:\n","\n","        # -------- Horizontal Gradient (Gx) --------\n","        gx = (\n","            -img[row-1, col-1] + img[row-1, col+1]\n","            -2*img[row,   col-1] + 2*img[row,   col+1]\n","            -img[row+1, col-1] + img[row+1, col+1]\n","        )\n","\n","        # -------- Vertical Gradient (Gy) --------\n","        gy = (\n","            -img[row-1, col-1] - 2*img[row-1, col] - img[row-1, col+1]\n","            + img[row+1, col-1] + 2*img[row+1, col] + img[row+1, col+1]\n","        )\n","\n","        # Edge magnitude\n","        out[row, col] = (gx*gx + gy*gy)**0.5\n","\n","def sobel_opencv(img):\n","    \"\"\"OpenCV CPU version using Sobel\"\"\"\n","    gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n","    gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)\n","    return np.sqrt(gx**2 + gy**2)\n","\n","  # Load 4K image from internet\n","urllib.request.urlretrieve(\"https://picsum.photos/3840/2160\", \"image.jpg\")\n","img = Image.open(\"image.jpg\").convert('L')   # Convert to grayscale\n","img = np.array(img, dtype=np.float32)\n","\n","H, W = img.shape\n","print(f\"Image: {W}x{H} ({W*H:,} pixels)\")\n","\n","# Copiar imagen a la GPU\n","d_img = cuda.to_device(img)\n","d_out = cuda.to_device(np.zeros_like(img))\n","\n","# Configuración de grid y bloques\n","threads = (32, 32)\n","blocks = ((W + 15) // 16, (H + 15) // 16)\n","\n","print(f\"Grid: {blocks} blocks x {threads} threads\")\n","\n","# Warmup (primer lanzamiento para “despertar” la GPU)\n","sobel_kernel[blocks, threads](d_img, d_out)\n","cuda.synchronize()\n","\n","# --- Ejecución cronometrada en GPU ---\n","start = time.time()\n","sobel_kernel[blocks, threads](d_img, d_out)\n","cuda.synchronize()\n","gpu_time = (time.time() - start) * 1000  # ms\n","\n","# Copiar resultado de la GPU a la CPU\n","out_gpu = d_out.copy_to_host()\n","\n","# --- Ejecución cronometrada en CPU (OpenCV) ---\n","start = time.time()\n","out_cpu = sobel_opencv(img)\n","cpu_time = (time.time() - start) * 1000  # ms\n","\n","# Results\n","print(\"\\n\" + \"=\"*60)\n","print(\"Results\")\n","print(\"=\"*60)\n","print(f\"GPU: {gpu_time:.2f} ms\")\n","print(f\"CPU: {cpu_time:.2f} ms\")\n","print(f\"Speedup: {cpu_time/gpu_time:.1f}x\")\n","print(f\"Correct: {np.allclose(out_gpu, out_cpu, atol=1e-3)}\")\n","\n","# Resize for display\n","H, W = img.shape\n","target_w = 256\n","target_h = int(target_w * H / W)\n","\n","def resize_for_plot(array):\n","    normalized = (array / array.max() * 255).astype(np.uint8)\n","    return np.array(Image.fromarray(normalized).resize((target_w, target_h), Image.LANCZOS))\n","\n","plt.figure(figsize=(20, 10))\n","\n","plt.subplot(1, 3, 1)\n","plt.imshow(resize_for_plot(img), cmap='gray')\n","plt.title('Original Image')\n","plt.axis('off')\n","\n","plt.subplot(1, 3, 2)\n","plt.imshow(resize_for_plot(out_gpu), cmap='gray')\n","plt.title('GPU Sobel Edges')\n","plt.axis('off')\n","\n","plt.subplot(1, 3, 3)\n","plt.imshow(resize_for_plot(out_cpu), cmap='gray')\n","plt.title('OpenCV CPU Sobel Edges')\n","plt.axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"cellView":"form","id":"YwmSfC3Pus8V"},"execution_count":null,"outputs":[]}]}